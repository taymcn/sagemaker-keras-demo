{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker is a modular, fully managed Machine Learning service that lets you easily build, train and deploy models at any scale.\n",
    "\n",
    "In this notebook, we'll use Keras (with the TensorFlow backend) to build a simple Convolutional Neural Network (CNN). We'll then train it to classify the Fashion-MNIST image data set. Fashion-MNIST is a Zalando dataset consisting of a training set of 60,000 examples and a validation set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: it's a drop-in replacement for MNIST.\n",
    "\n",
    "This notebook modified from AIM410R/R1 session at AWS re:Invent 2019. https://gitlab.com/juliensimon/aim410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  * Amazon SageMaker documentation [ https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html ]\n",
    "  * SageMaker SDK \n",
    "    * Code [ https://github.com/aws/sagemaker-python-sdk ] \n",
    "    * Documentation [ https://sagemaker.readthedocs.io/ ]\n",
    "  * Fashion-MNIST [ https://github.com/zalandoresearch/fashion-mnist ] \n",
    "  * Keras documentation [ https://keras.io/ ]\n",
    "  * Numpy documentation [ https://docs.scipy.org/doc/numpy/index.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the latest SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install --upgrade pip\n",
    "pip install smdebug smdebug-rulesconfig==0.1.2\n",
    "# pip -q install sagemaker smdebug smdebug-rulesconfig==0.1.2 awscli boto3 keras --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"fashion-mnist-sprite.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data set from the Internet. Fortunately, Keras provides a simple way to do this. The data set is already split (training and validation), with separate Numpy arrays for samples and labels. \n",
    "\n",
    "We create a local directory, and save the training and validation data sets separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = fashion_mnist.load_data()\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "\n",
    "np.savez('./data/training', image=x_train, label=y_train)\n",
    "np.savez('./data/validation', image=x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at our Keras code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pygmentize mnist_keras_tf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps are:\n",
    "  * receive and parse command line arguments: five hyper parameters, and four environment variables (we'll get back to these in a moment)\n",
    "  * load the data sets\n",
    "  * make sure data sets have the right shape for TensorFlow (channels last)\n",
    "  * normalize data sets, i.e. tranform [0-255] pixel values to [0-1] values\n",
    "  * one-hot encode category labels (not familiar with this? More info: [ https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/ ])\n",
    "  * Build a Sequential model in Keras: two convolution block with max pooling, followed by a fully connected layer with dropout, and a final classification layer. Don't worry if this sounds like gibberish, it's not our focus today\n",
    "  * Train the model, leveraging multiple GPUs if they're available.\n",
    "  * Print statistics\n",
    "  * Save the model in TensorFlow serving format\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train outside of SageMaker (just like on your laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training on SageMaker, let's run this code locally and make sure that it trains fine. We just need to set the four environment variables that it expects, and run the script with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of GPUs on this machine\n",
    "# %env SM_NUM_GPUS=0\n",
    "# # Where to save the model\n",
    "# %env SM_MODEL_DIR=/tmp/model\n",
    "# # Where the training data is\n",
    "# %env SM_CHANNEL_TRAINING=data\n",
    "# # Where the validation data is\n",
    "# %env SM_CHANNEL_VALIDATION=data\n",
    "\n",
    "# !/bin/rm -rf $SM_MODEL_DIR\n",
    "# !python mnist_keras_tf.py --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are these environment variables important anyway? Well, they will be automatically passed to our script by SageMaker, so that we know where the data sets are, where to save the model, and how many GPUs we have. So, if you write your local code this way, **there won't be anything to change** to run it on SageMaker.\n",
    "\n",
    "This feature is called '**script mode**', it's the recommended way to work with built-in frameworks on SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on the notebook instance (aka 'local mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code runs fine. Now, let's try to run it inside the built-in TensorFlow environment provided by SageMaker. For fast experimentation, let's use local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='local',\n",
    "                          framework_version='1.15', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 1}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the local location of the training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_training_input_path   = 'file://data/training.npz'\n",
    "local_validation_input_path = 'file://data/validation.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': local_training_input_path, 'validation': local_validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our job runs fine locally. Let's now run the same job on a managed instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data set to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker training instances expect data sets to be stored in Amazon S3, so let's upload them there. We could use boto3 to do this, but the SageMaker SDK includes a simple function: [Session.upload_data()](https://sagemaker.readthedocs.io/en/stable/session.html).\n",
    "\n",
    "\n",
    "\n",
    "*Note: for high-performance workloads, Amazon EFS and Amazon FSx for Lustre are now also supported. More info [here](https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'keras-fashion-mnist'\n",
    "\n",
    "# Upload the training data set to 'keras-fashion-mnist/training'\n",
    "training_input_path   = sess.upload_data('data/training.npz', key_prefix=prefix+'/training')\n",
    "\n",
    "# Upload the validation data set to 'keras-fashion-mnist/validation'\n",
    "validation_input_path = sess.upload_data('data/validation.npz', key_prefix=prefix+'/validation')\n",
    "\n",
    "print(training_input_path)\n",
    "print(validation_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with our data set. Of course, in real life, much more work would be needed for data cleaning and preparation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Managed Spot Training, and enable debugging with Amazon SageMaker Debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EC2 Spot Instances have long been a great cost optimization feature, and spot training is now available on SageMaker.\n",
    "This blog [post](https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/) has more info.\n",
    "\n",
    "We're also using Amazon SageMaker Debugger to check for unwanted training conditions. **ZERO KERAS CODE NEEDED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a managed training job for 'mnist_keras_tf.py', \n",
    "# using a single c5.2xlarge spot instance running TensorFlow 1.15 in script mode\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.p3.2xlarge',\n",
    "                          framework_version='1.15', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          train_use_spot_instances=True,        # Use spot instance\n",
    "                          train_max_run=600,                    # Max training time\n",
    "                          train_max_wait=3600,                  # Max training time + spot waiting time\n",
    "                          rules = [Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "                                   Rule.sagemaker(rule_configs.overfit())]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the training and validation data sets stored in S3\n",
    "\n",
    "tf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take about 10 minutes. Please take a look at the training log. The first few lines show SageMaker preparing the managed instance. While the job is training, you can also look at metrics in the AWS console for SageMaker, and at the training log in the the AWS console for CloudWatch Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of the debug rules we configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = tf_estimator.latest_training_job.name\n",
    "client = tf_estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "for status in description['DebugRuleEvaluationStatuses']:\n",
    "    status.pop('LastModifiedTime')\n",
    "    status.pop('RuleEvaluationJobArn')\n",
    "    pprint.pprint(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at tensor information saved in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path = description[\"DebugHookConfig\"][\"S3OutputPath\"] + job_name + '/' + 'debug-output/'\n",
    "\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$s3_output_path\"\n",
    "\n",
    "aws s3 ls --recursive $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smdebug\n",
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial = create_trial(s3_output_path)\n",
    "trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = trial.tensor('loss').values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic model tuning is a great feature that helps you find automatically the best hyper parameters for your training job.\n",
    "\n",
    "This blog [post](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/) has more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges :\n",
    "\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'epochs' :       IntegerParameter(5, 20),\n",
    "    'learning-rate': ContinuousParameter(0.001, 0.1, scaling_type='ReverseLogarithmic'), \n",
    "    'batch-size':    IntegerParameter(32, 1024),\n",
    "    'filters':       IntegerParameter(4, 64),\n",
    "    'dense-layer':   IntegerParameter(32, 1024),\n",
    "    'dropout':       ContinuousParameter(0.2, 0.8)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the metric we're optimizing for, in this case we want to maximize the validation accuracy. We also grab other metrics from the training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'validation_accuracy'\n",
    "\n",
    "objective_type = 'Maximize'\n",
    "\n",
    "metric_definitions = [\n",
    "    {'Name': 'training_loss',        'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_accuracy',    'Regex': 'acc: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_loss',      'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_accuracy',  'Regex': 'val_acc: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_precision',   'Regex': 'precision: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_recall',      'Regex': 'recall: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_f1_score',    'Regex': 'f1_score: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_precision', 'Regex': 'val_precision: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_recall',    'Regex': 'val_recall: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_f1_score',  'Regex': 'val_f1_score: ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it's time to put everything together, and configure the tuning job. Same estimator as above, without the debugging job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.p3.2xlarge',\n",
    "                          framework_version='1.15', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          train_use_spot_instances=True,        # Use spot instance\n",
    "                          train_max_run=600,                    # Max training time\n",
    "                          train_max_wait=3600                   # Max training time + spot waiting time\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "# Configure a training job using the Tensorflow estimator, the parameter ranges and the metric defined above.\n",
    "# Let's run ten individual jobs, two by two.\n",
    "\n",
    "tuner = HyperparameterTuner(tf_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=3,\n",
    "                            max_parallel_jobs=1,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the tuning job, just like a normal estimator. We definitely want to use spot training here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the tuning job, passing the location of the data sets in S3.\n",
    "\n",
    "tuner.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the job is running, you can view it in the AWS console for SageMaker: individual jobs (and their logs), best training job so far, etc.\n",
    "\n",
    "Of course, you can also inspect the job programatically using [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) : *decribe_hyper_parameter_training_job()*, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect jobs with Amazon SageMaker Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tuning automatically creates a new experiment, and pushes information for each job. \n",
    "\n",
    "**ZERO KERAS CODE NEEDED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "exp = HyperparameterTuningJobAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    hyperparameter_tuning_job_name=tuner.latest_tuning_job.name\n",
    ")\n",
    "\n",
    "df = exp.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is the Swiss army knife for columnar data. Let's just look at the top job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job = df.sort_values('FinalObjectiveValue', ascending=0)[:1]\n",
    "best_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job_name = best_job['TrainingJobName'].to_string(index=False).strip()\n",
    "best_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job = sm.describe_training_job(TrainingJobName=best_job_name)\n",
    "\n",
    "best_model_artefact = best_job['ModelArtifacts']['S3ModelArtifacts']\n",
    "best_model_container = best_job['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "print(best_job_name)\n",
    "print(best_model_artefact)\n",
    "print(best_model_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the best model, enabling data capture with Amazon SageMaker Model Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we want to save captured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()\n",
    "prefix = '/ModelMonitorDEMO/'\n",
    "s3_capture_path = 's3://' + bucket + prefix + best_job_name + '/'\n",
    "\n",
    "print(s3_capture_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we will capture 100% of model inputs and outputs. Of course, this is configurable.\n",
    "\n",
    "And you guessed it... **ZERO KERAS CODE NEEDED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "cap = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    destination_s3_uri=s3_capture_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = best_job_name + '-ep'\n",
    "\n",
    "best_model_predictor = tuner.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m5.xlarge', \n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 10\n",
    "indices = random.sample(range(x_val.shape[0] - 1), num_samples)\n",
    "images = x_val[indices]/255\n",
    "labels = y_val[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "prediction = best_model_predictor.predict(images.reshape(num_samples, 28, 28, 1))['predictions']\n",
    "prediction = np.array(prediction)\n",
    "predicted_labels = prediction.argmax(axis=1)\n",
    "print('Predicted labels are: {}'.format(predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict the validation dataset 250 samples at a time, storing labels and predicted labels as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_samples = 250\n",
    "all_labels=[]\n",
    "all_predicted_labels=[]\n",
    "\n",
    "import sys\n",
    "\n",
    "for i in range(0,x_val.shape[0] - 1,num_samples):\n",
    "    sys.stdout.write(str(i)+' ')\n",
    "    indices = range(i,i+num_samples)\n",
    "    images = x_val[indices]/255\n",
    "    labels = y_val[indices]\n",
    "    prediction = best_model_predictor.predict(images.reshape(num_samples, 28, 28, 1))['predictions']\n",
    "    prediction = np.array(prediction)\n",
    "    predicted_labels = prediction.argmax(axis=1)\n",
    "    all_labels.extend(labels)\n",
    "    all_predicted_labels.extend(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the confusion matrix, to compare predicted labels with real labels for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predicted_labels)\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "fmt = 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] < thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "classes = range(10) # Labels are sorted \n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we captured data (you may have to wait a minute or two for files to show up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$s3_capture_path\"\n",
    "\n",
    "aws s3 ls --recursive $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$s3_capture_path\"\n",
    "\n",
    "aws s3 cp --recursive $1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy file name from above (\"tensorflow-training...jsonl\") and paste below to preview.\n",
    "# Example:\n",
    "# !head tensorflow-training-200922-0403-001-a3ab0f09-ep/AllTraffic/2020/09/22/04/23-33-408-af4b5c9d-540a-4fcc-9105-dc0eae6e417b.jsonl\n",
    "\n",
    "!head tensorflow-training-200922-0403-001-a3ab0f09-ep/AllTraffic/2020/09/22/04/23-33-408-af4b5c9d-540a-4fcc-9105-dc0eae6e417b.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client('sagemaker')\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
