{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker is a modular, fully managed Machine Learning service that lets you easily build, train and deploy models at any scale.\n",
    "\n",
    "In this demo, we demonstrate using SageMaker's script mode, managed spot training, Debugger, Automatic Model Tuning, Experiments, and Model Monitor features.\n",
    "\n",
    "We'll use Keras with the TensorFlow backend to build a simple Convolutional Neural Network (CNN) on Amazon SageMaker and train it to classify the Fashion-MNIST image data set.\n",
    "\n",
    "Fashion-MNIST is a Zalando dataset consisting of a training set of 60,000 examples and a validation set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: it's a drop-in replacement for MNIST.\n",
    "\n",
    "Demo modified from AIM410R/R1 session at AWS re:Invent 2019. https://gitlab.com/juliensimon/aim410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  * Amazon SageMaker documentation [ https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html ]\n",
    "  * SageMaker SDK \n",
    "    * Code [ https://github.com/aws/sagemaker-python-sdk ] \n",
    "    * Documentation [ https://sagemaker.readthedocs.io/ ]\n",
    "  * Fashion-MNIST [ https://github.com/zalandoresearch/fashion-mnist ] \n",
    "  * Keras documentation [ https://keras.io/ ]\n",
    "  * Numpy documentation [ https://docs.scipy.org/doc/numpy/index.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install --upgrade pip\n",
    "pip install smdebug smdebug-rulesconfig # install SageMaker Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = None # Specify bucket or leave 'None' to use default\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"Session bucket: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"fashion-mnist-sprite.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data set from the Internet. Fortunately, Keras provides a simple way to do this. The data set is already split (training and validation), with separate Numpy arrays for samples and labels. \n",
    "\n",
    "We create a local directory, and save the training and validation data sets separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = fashion_mnist.load_data()\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "\n",
    "np.savez('./data/training', image=x_train, label=y_train)\n",
    "np.savez('./data/validation', image=x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at our Keras code\n",
    "\n",
    "What are these environment variables and why are they important? Well, they will be automatically passed to our script by SageMaker, so that we know where the data sets are, where to save the model, and how many GPUs we have. So, if you write your code this way, **there won't be anything to change** to run it on SageMaker.\n",
    "\n",
    "This feature is called '**script mode**', it's the recommended way to work with built-in frameworks on SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pygmentize mnist_keras_tf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps are:\n",
    "  * receive and parse command line arguments: five hyper parameters, and four environment variables\n",
    "  * load the data sets\n",
    "  * make sure data sets have the right shape for TensorFlow (channels last)\n",
    "  * normalize data sets, i.e. tranform [0-255] pixel values to [0-1] values\n",
    "  * one-hot encode category labels (not familiar with this? More info: [ https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/ ])\n",
    "  * Build a Sequential model in Keras: two convolution block with max pooling, followed by a fully connected layer with dropout, and a final classification layer. Don't worry if this sounds like gibberish, it's not our focus today\n",
    "  * Train the model, leveraging multiple GPUs if they're available.\n",
    "  * Print statistics\n",
    "  * Save the model in TensorFlow serving format\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Tensorflow on the notebook instance (aka 'local mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code inside the built-in TensorFlow environment provided by SageMaker. For fast experimentation, let's use local mode to train on the local notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='mnist_keras_tf.py',\n",
    "                          output_path=f\"s3://{sess.default_bucket()}\",\n",
    "                          role=role,\n",
    "                          instance_count=1, \n",
    "                          instance_type='local',\n",
    "                          framework_version='1.15', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 1}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the local location of the training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_training_input_path   = 'file://data/training.npz'\n",
    "local_validation_input_path = 'file://data/validation.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': local_training_input_path, 'validation': local_validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our job runs fine locally. Let's now run the same job on a managed instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data set to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker training instances expect data sets to be stored in Amazon S3, so let's upload them there. We could use boto3 to do this, but the SageMaker SDK includes a simple function: [Session.upload_data()](https://sagemaker.readthedocs.io/en/stable/session.html).\n",
    "\n",
    "\n",
    "\n",
    "*Note: for high-performance workloads, Amazon EFS and Amazon FSx for Lustre are now also supported. More info [here](https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'keras-fashion-mnist'\n",
    "\n",
    "# Upload the training data set to 'keras-fashion-mnist/training'\n",
    "training_input_path   = sess.upload_data('data/training.npz', key_prefix=prefix+'/training')\n",
    "\n",
    "# Upload the validation data set to 'keras-fashion-mnist/validation'\n",
    "validation_input_path = sess.upload_data('data/validation.npz', key_prefix=prefix+'/validation')\n",
    "\n",
    "print(training_input_path)\n",
    "print(validation_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with our data set. Of course, in real life, much more work would be needed for data cleaning and preparation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Managed Spot Training, and enable debugging with Amazon SageMaker Debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EC2 Spot Instances have long been a great cost optimization feature, and spot training is now available on SageMaker.\n",
    "This blog [post](https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/) has more info.\n",
    "\n",
    "We're also using Amazon SageMaker Debugger to check for unwanted training conditions. **ZERO KERAS CODE NEEDED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a managed training job for 'mnist_keras_tf.py', \n",
    "# using a single p3.2xlarge instance running TensorFlow 1.15 in script mode\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n",
    "                          output_path=f\"s3://{sess.default_bucket()}\",\n",
    "                          role=role,\n",
    "                          instance_count=1, \n",
    "                          instance_type='ml.p3.2xlarge',\n",
    "                          framework_version='1.15', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 5},\n",
    "                          use_spot_instances=True,        # Use spot instance\n",
    "                          max_run=480,                    # Max training time\n",
    "                          max_wait=720,                  # Max training time + spot waiting time\n",
    "                          rules = [Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "                                   Rule.sagemaker(rule_configs.overfit())]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the training and validation data sets stored in S3\n",
    "\n",
    "tf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take about 10 minutes. Please take a look at the training log. The first few lines show SageMaker preparing the managed instance. While the job is training, you can also look at metrics in the AWS console for SageMaker, and at the training log in the the AWS console for CloudWatch Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of the debug rules we configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = tf_estimator.latest_training_job.name\n",
    "client = tf_estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "for status in description['DebugRuleEvaluationStatuses']:\n",
    "    status.pop('LastModifiedTime')\n",
    "    status.pop('RuleEvaluationJobArn')\n",
    "    pprint.pprint(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at tensor information saved in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path = f\"s3://{sess.default_bucket()}/{job_name}/debug-output\"\n",
    "\n",
    "print(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smdebug\n",
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial = create_trial(s3_output_path)\n",
    "trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = trial.tensor('loss').values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic model tuning is a great feature that helps you find automatically the best hyper parameters for your training job.\n",
    "\n",
    "This blog [post](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/) has more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define parameter ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges :\n",
    "\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'learning-rate': ContinuousParameter(0.001, 0.1, scaling_type='ReverseLogarithmic'), \n",
    "    'batch-size':    IntegerParameter(32, 1024),\n",
    "    'filters':       IntegerParameter(4, 64),\n",
    "    'dense-layer':   IntegerParameter(32, 1024),\n",
    "    'dropout':       ContinuousParameter(0.2, 0.8)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the metric we're optimizing for, in this case we want to maximize the validation accuracy. We also grab other metrics from the training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'validation_accuracy'\n",
    "\n",
    "objective_type = 'Maximize'\n",
    "\n",
    "metric_definitions = [\n",
    "    {'Name': 'training_loss',        'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_accuracy',    'Regex': 'acc: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_loss',      'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_accuracy',  'Regex': 'val_acc: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_precision',   'Regex': 'precision: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_recall',      'Regex': 'recall: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'training_f1_score',    'Regex': 'f1_score: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_precision', 'Regex': 'val_precision: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_recall',    'Regex': 'val_recall: ([0-9\\\\.]+)'},\n",
    "    {'Name': 'validation_f1_score',  'Regex': 'val_f1_score: ([0-9\\\\.]+)'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it's time to put everything together, and configure the tuning job. Same estimator as above, without the debugging job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n",
    "                          output_path=f\"s3://{sess.default_bucket()}\",\n",
    "                          role=role,\n",
    "                          instance_count=1, \n",
    "                          instance_type='ml.p3.2xlarge',\n",
    "                          framework_version='1.15', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 5}\n",
    "#                           use_spot_instances=True,        # Use spot instance\n",
    "#                           max_run=600,                    # Max training time\n",
    "#                           max_wait=720                   # Max training time + spot waiting time\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "# Configure a training job using the Tensorflow estimator, the parameter ranges and the metric defined above.\n",
    "# Let's run four individual jobs, two by two.\n",
    "\n",
    "tuner = HyperparameterTuner(tf_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=4,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the tuning job, just like a normal estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the tuning job, passing the location of the data sets in S3.\n",
    "\n",
    "tuner.fit({'training': training_input_path, 'validation': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the job is running, you can view it in the AWS console for SageMaker: individual jobs (and their logs), best training job so far, etc.\n",
    "\n",
    "Of course, you can also inspect the job programatically using [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) : *decribe_hyper_parameter_training_job()*, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect jobs with Amazon SageMaker Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tuning automatically creates a new experiment, and pushes information for each job. \n",
    "\n",
    "**ZERO KERAS CODE NEEDED!**\n",
    "\n",
    "Run the following cell to see the status of the automatic model tuning job. It may take a minute for the initial job to appear. Try rerunning the cell to see updated job statuses. \n",
    "\n",
    "Note TrainingJobStatus may be 'InProgress' for one or more jobs. Status is 'Completed' when jobs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "exp = HyperparameterTuningJobAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    hyperparameter_tuning_job_name=tuner.latest_tuning_job.name\n",
    ")\n",
    "\n",
    "df = exp.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is the Swiss army knife for columnar data. Let's just look at the top job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job = df.sort_values('FinalObjectiveValue', ascending=0)[:1]\n",
    "best_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job_name = best_job['TrainingJobName'].to_string(index=False).strip()\n",
    "best_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job = sm.describe_training_job(TrainingJobName=best_job_name)\n",
    "\n",
    "best_model_artefact = best_job['ModelArtifacts']['S3ModelArtifacts']\n",
    "best_model_container = best_job['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "print(best_job_name)\n",
    "print(best_model_artefact)\n",
    "print(best_model_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the best model, enabling data capture with Amazon SageMaker Model Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we want to save captured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/ModelMonitorDEMO/'\n",
    "s3_capture_path = 's3://' + sess.default_bucket() + prefix + best_job_name + '/'\n",
    "\n",
    "print(s3_capture_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we will capture 100% of model inputs and outputs. Of course, this is configurable.\n",
    "\n",
    "And you guessed it... **ZERO KERAS CODE NEEDED!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "cap = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    destination_s3_uri=s3_capture_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = best_job_name + '-ep'\n",
    "\n",
    "best_model_predictor = tuner.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m5.xlarge', \n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 10\n",
    "indices = random.sample(range(x_val.shape[0] - 1), num_samples)\n",
    "images = x_val[indices]/255\n",
    "labels = y_val[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "prediction = best_model_predictor.predict(images.reshape(num_samples, 28, 28, 1))['predictions']\n",
    "prediction = np.array(prediction)\n",
    "predicted_labels = prediction.argmax(axis=1)\n",
    "print('Predicted labels are: {}'.format(predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict the validation dataset 250 samples at a time, storing labels and predicted labels as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_samples = 250\n",
    "all_labels=[]\n",
    "all_predicted_labels=[]\n",
    "\n",
    "import sys\n",
    "\n",
    "for i in range(0,x_val.shape[0] - 1,num_samples):\n",
    "    sys.stdout.write(str(i)+' ')\n",
    "    indices = range(i,i+num_samples)\n",
    "    images = x_val[indices]/255\n",
    "    labels = y_val[indices]\n",
    "    prediction = best_model_predictor.predict(images.reshape(num_samples, 28, 28, 1))['predictions']\n",
    "    prediction = np.array(prediction)\n",
    "    predicted_labels = prediction.argmax(axis=1)\n",
    "    all_labels.extend(labels)\n",
    "    all_predicted_labels.extend(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the confusion matrix, to compare predicted labels with real labels for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predicted_labels)\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "fmt = 'd'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] < thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "classes = range(10) # Labels are sorted \n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we captured data (you may have to wait a minute or two for files to show up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$s3_capture_path\"\n",
    "\n",
    "aws s3 ls --recursive $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s \"$s3_capture_path\"\n",
    "\n",
    "aws s3 cp --recursive $1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy local file name from the cell output above (\"tensorflow-training...jsonl\") and paste below to preview.\n",
    "# Your code should look like:\n",
    "# !head tensorflow-training-200922-0403-001-a3ab0f09-ep/AllTraffic/2020/09/22/04/23-33-408-af4b5c9d-540a-4fcc-9105-dc0eae6e417b.jsonl\n",
    "\n",
    "!head #REPLACE ME WITH YOUR FILE NAME#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client('sagemaker')\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
